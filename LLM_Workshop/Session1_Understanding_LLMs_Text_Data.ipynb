{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ“ Session 1: Understanding LLMs & Working with Text Data\n",
                "\n",
                "**Workshop: Build a Large Language Model From Scratch**  \n",
                "**Duration**: 3 hours (9:30 AM - 12:30 PM)  \n",
                "**Instructor Notes**: ðŸ“¢ indicates what to say, ðŸ’¡ indicates key points to emphasize\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“‹ Session Overview\n",
                "\n",
                "| Time | Topic | Type |\n",
                "|------|-------|------|\n",
                "| 9:30 - 10:15 | Introduction to LLMs | Theory |\n",
                "| 10:15 - 10:30 | Break | - |\n",
                "| 10:30 - 11:15 | Tokenization | Hands-on |\n",
                "| 11:15 - 12:00 | Embeddings & DataLoader | Hands-on |\n",
                "| 12:00 - 12:30 | Exercises & Q&A | Practice |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part 1: Introduction to Large Language Models (45 min)\n",
                "---\n",
                "\n",
                "## ðŸ“¢ Instructor Script\n",
                "\n",
                "> \"Welcome everyone! Today we're going to build something amazing - we'll understand and code a Large Language Model from scratch, similar to what powers ChatGPT!\"\n",
                "\n",
                "> \"By the end of this workshop, you'll understand exactly how these models work under the hood.\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ¤” What is a Large Language Model?\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"A Large Language Model is essentially a neural network trained on massive amounts of text data to predict the next word in a sequence. That's the core idea - next word prediction!\"\n",
                "\n",
                "ðŸ’¡ **Key Points:**\n",
                "- LLMs are trained on billions of text tokens (words/subwords)\n",
                "- They learn patterns, grammar, facts, and even reasoning\n",
                "- The \"Large\" refers to billions of parameters (weights)\n",
                "- GPT-2 has 124M-1.5B parameters, GPT-3 has 175B, GPT-4 is estimated at 1.7T+"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“œ Brief History: How We Got Here\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Let me give you a quick history. Before 2017, we used RNNs and LSTMs for language tasks. They had a problem - they processed text sequentially and struggled with long sequences.\"\n",
                "\n",
                "**Timeline:**\n",
                "\n",
                "| Year | Milestone |\n",
                "|------|----------|\n",
                "| Pre-2017 | RNNs, LSTMs - sequential processing, memory issues |\n",
                "| 2017 | \"Attention Is All You Need\" paper - Transformers! |\n",
                "| 2018 | GPT-1 by OpenAI (117M parameters) |\n",
                "| 2019 | GPT-2 (1.5B parameters) - \"too dangerous to release\" |\n",
                "| 2020 | GPT-3 (175B parameters) |\n",
                "| 2022 | ChatGPT - made LLMs mainstream |\n",
                "| 2023 | GPT-4, Llama 2, Claude 2, Mistral 7B |\n",
                "| 2024 | GPT-4o, Claude 3.5 Sonnet, Gemini 1.5, Llama 3, DeepSeek-V2 |\n",
                "| 2025 | DeepSeek-R1, GPT-o3, Llama 4, Gemini 2.0 - open-source models rival closed ones |\n",
                "\n",
                "ðŸ’¡ **Key Point:** The secret sauce was **ATTENTION** - allowing the model to look at all words simultaneously!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ§  How LLMs Work: The 10,000-Foot View\n",
                "\n",
                "ðŸ“¢ **Say this (draw on whiteboard if available):**\n",
                "> \"Here's the magic: You give the model some text, it predicts what comes next. Do this billions of times with internet text, and the model learns language!\"\n",
                "\n",
                "```\n",
                "Input: \"The cat sat on the\"\n",
                "                    â†“\n",
                "           [LLM Processing]\n",
                "                    â†“\n",
                "Output: \"mat\" (with 95% probability)\n",
                "        \"floor\" (with 3% probability)\n",
                "        \"dog\" (with 0.5% probability)\n",
                "        ...\n",
                "```\n",
                "\n",
                "ðŸ’¡ **Key Insight:** Generation is just repeated next-token prediction!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ—ï¸ What We'll Build in This Workshop\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Over the next 2 days, we're going to build a GPT model from absolute scratch. Here's our roadmap:\"\n",
                "\n",
                "**Day 1:**\n",
                "- Session 1 (now): Text processing - how to convert text to numbers\n",
                "- Session 2: Attention mechanisms - the heart of transformers\n",
                "\n",
                "**Day 2:**\n",
                "- Session 3: Full GPT architecture - putting it all together\n",
                "- Session 4: Training and finetuning - making it actually work\n",
                "\n",
                "```\n",
                "Text â†’ Tokens â†’ Embeddings â†’ Attention â†’ Transformer â†’ Output\n",
                " â†‘                                                        â†‘\n",
                " |__________________ Today's Focus _______________________|\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## â˜• BREAK TIME (10:15 - 10:30)\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part 2: Working with Text Data - Tokenization (45 min)\n",
                "---\n",
                "\n",
                "## ðŸ“¢ Instructor Script\n",
                "> \"Welcome back! Now let's get our hands dirty with code. The first problem we need to solve is: How do we convert text into numbers that a neural network can process?\"\n",
                "\n",
                "> \"We can't just feed 'Hello World' to a neural network - we need numbers!\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ”§ Setup & Environment Check\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Let's first make sure everyone's environment is set up correctly. Run this cell:\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install tiktoken matplotlib numpy requests transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Python version: 3.13.2 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:14) [MSC v.1929 64 bit (AMD64)]\n",
                        "PyTorch version: 2.8.0+cu129\n",
                        "Tiktoken version: 0.12.0\n",
                        "\n",
                        "âœ… All set! Let's begin!\n"
                    ]
                }
            ],
            "source": [
                "# Check our environment\n",
                "import sys\n",
                "print(f\"Python version: {sys.version}\")\n",
                "\n",
                "# Import required libraries\n",
                "import torch\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "\n",
                "import tiktoken\n",
                "print(f\"Tiktoken version: {tiktoken.__version__}\")\n",
                "\n",
                "print(\"\\nâœ… All set! Let's begin!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“ What is Tokenization?\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Tokenization is splitting text into smaller pieces called tokens. Think of tokens as the 'atoms' of text for our model.\"\n",
                "\n",
                "ðŸ’¡ **Key Points:**\n",
                "- A token can be a word, part of a word, or even a single character\n",
                "- Different strategies exist: word-level, character-level, subword-level\n",
                "- Modern LLMs use **subword tokenization** (like BPE - Byte Pair Encoding)\n",
                "\n",
                "```\n",
                "\"Hello, world!\" \n",
                "     â†“ (tokenization)\n",
                "[\"Hello\", \",\", \" world\", \"!\"]\n",
                "     â†“ (convert to IDs)\n",
                "[15496, 11, 995, 0]\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ› ï¸ Building a Simple Tokenizer from Scratch\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Let's build a simple tokenizer first to understand the concept. We'll use regular expressions to split text.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Naive split: ['Hello,', 'world.', 'This', 'is', 'a', 'test!']\n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "\n",
                "# Simple text for demonstration\n",
                "text = \"Hello, world. This is a test!\"\n",
                "\n",
                "# Step 1: Split on whitespace (naive approach)\n",
                "simple_split = text.split()\n",
                "print(\"Naive split:\", simple_split)\n",
                "\n",
                "# Problem: punctuation is attached to words!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ðŸ“¢ **Say this:**\n",
                "> \"Notice the problem? 'Hello,' and 'test!' have punctuation attached. We need to separate those.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Better tokenization: ['Hello', ',', 'world', '.', 'This', 'is', 'a', 'test', '!']\n"
                    ]
                }
            ],
            "source": [
                "# Step 2: Better tokenization with regex\n",
                "# This pattern splits on whitespace AND separates punctuation\n",
                "pattern = r'([,.:;?_!\"()\\']|--|\\s)'\n",
                "\n",
                "tokens = re.split(pattern, text)\n",
                "# Remove empty strings and whitespace-only tokens\n",
                "tokens = [t.strip() for t in tokens if t.strip()]\n",
                "\n",
                "print(\"Better tokenization:\", tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ðŸ’¡ **Explain the regex:**\n",
                "> \"This regex pattern does two things:\n",
                "> 1. `[,.:;?_!\"()']` - matches common punctuation\n",
                "> 2. `--` - matches em-dashes\n",
                "> 3. `\\s` - matches whitespace\n",
                "> The parentheses `()` keep the delimiters in our results.\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“š Loading Real Text Data\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Now let's work with some real text. We'll use 'The Verdict' by Edith Wharton - a public domain short story.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total characters: 20,479\n",
                        "\n",
                        "First 200 characters:\n",
                        "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import requests\n",
                "\n",
                "# Download the text file\n",
                "if not os.path.exists(\"the-verdict.txt\"):\n",
                "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
                "    response = requests.get(url)\n",
                "    with open(\"the-verdict.txt\", \"w\", encoding=\"utf-8\") as f:\n",
                "        f.write(response.text)\n",
                "    print(\"Downloaded the-verdict.txt\")\n",
                "\n",
                "# Read the text\n",
                "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
                "    raw_text = f.read()\n",
                "\n",
                "print(f\"Total characters: {len(raw_text):,}\")\n",
                "print(f\"\\nFirst 200 characters:\\n{raw_text[:200]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total tokens: 4,690\n",
                        "First 30 tokens: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
                    ]
                }
            ],
            "source": [
                "# Tokenize the entire text\n",
                "pattern = r'([,.:;?_!\"()\\']|--|\\s)'\n",
                "tokens = re.split(pattern, raw_text)\n",
                "tokens = [t.strip() for t in tokens if t.strip()]\n",
                "\n",
                "print(f\"Total tokens: {len(tokens):,}\")\n",
                "print(f\"First 30 tokens: {tokens[:30]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ”¢ Converting Tokens to IDs: Building a Vocabulary\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Now we have text split into tokens. Next step: assign each unique token a number (ID). This is our vocabulary!\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Vocabulary size: 1130\n",
                        "\n",
                        "First 20 vocabulary items: ['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be']\n"
                    ]
                }
            ],
            "source": [
                "# Create vocabulary: unique tokens sorted alphabetically\n",
                "all_words = sorted(set(tokens))\n",
                "vocab_size = len(all_words)\n",
                "\n",
                "print(f\"Vocabulary size: {vocab_size}\")\n",
                "print(f\"\\nFirst 20 vocabulary items: {all_words[:20]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sample vocabulary mappings:\n",
                        "  '!' â†’ 0\n",
                        "  '\"' â†’ 1\n",
                        "  ''' â†’ 2\n",
                        "  '(' â†’ 3\n",
                        "  ')' â†’ 4\n",
                        "  ',' â†’ 5\n",
                        "  '--' â†’ 6\n",
                        "  '.' â†’ 7\n",
                        "  ':' â†’ 8\n",
                        "  ';' â†’ 9\n"
                    ]
                }
            ],
            "source": [
                "# Create token-to-ID mapping\n",
                "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
                "\n",
                "# Show some mappings\n",
                "print(\"Sample vocabulary mappings:\")\n",
                "for token, idx in list(vocab.items())[:10]:\n",
                "    print(f\"  '{token}' â†’ {idx}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ­ Building a Complete Tokenizer Class\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Let's wrap this all up into a reusable class. A tokenizer needs two main functions: encode (text â†’ IDs) and decode (IDs â†’ text).\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleTokenizer:\n",
                "    \"\"\"A simple word-level tokenizer.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab):\n",
                "        self.str_to_int = vocab\n",
                "        self.int_to_str = {idx: token for token, idx in vocab.items()}\n",
                "    \n",
                "    def encode(self, text):\n",
                "        \"\"\"Convert text to token IDs.\"\"\"\n",
                "        # Tokenize\n",
                "        pattern = r'([,.:;?_!\"()\\']|--|\\s)'\n",
                "        tokens = re.split(pattern, text)\n",
                "        tokens = [t.strip() for t in tokens if t.strip()]\n",
                "        # Convert to IDs\n",
                "        ids = [self.str_to_int[token] for token in tokens]\n",
                "        return ids\n",
                "    \n",
                "    def decode(self, ids):\n",
                "        \"\"\"Convert token IDs back to text.\"\"\"\n",
                "        tokens = [self.int_to_str[idx] for idx in ids]\n",
                "        text = \" \".join(tokens)\n",
                "        # Clean up spaces before punctuation\n",
                "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
                "        return text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original: \"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
                        "\n",
                        "Encoded IDs: [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
                        "\n",
                        "Decoded: \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
                    ]
                }
            ],
            "source": [
                "# Test our tokenizer\n",
                "tokenizer = SimpleTokenizer(vocab)\n",
                "\n",
                "test_text = '\"It\\'s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'\n",
                "\n",
                "# Encode\n",
                "ids = tokenizer.encode(test_text)\n",
                "print(f\"Original: {test_text}\")\n",
                "print(f\"\\nEncoded IDs: {ids}\")\n",
                "\n",
                "# Decode\n",
                "decoded = tokenizer.decode(ids)\n",
                "print(f\"\\nDecoded: {decoded}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## âš ï¸ The Problem with Simple Tokenizers\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Our simple tokenizer has a big problem. What happens if we encounter a word that's not in our vocabulary?\"\n",
                "\n",
                "ðŸ’¡ **Key Issues:**\n",
                "1. Unknown words cause errors\n",
                "2. Vocabulary can get very large\n",
                "3. Rare words waste vocabulary space"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âŒ Error: 'Hello' is not in vocabulary!\n",
                        "\n",
                        "ðŸ’¡ This is why we need special tokens and better tokenization!\n"
                    ]
                }
            ],
            "source": [
                "# This will fail! \"Hello\" is not in our vocabulary\n",
                "try:\n",
                "    ids = tokenizer.encode(\"Hello, this is a completely new sentence!\")\n",
                "except KeyError as e:\n",
                "    print(f\"âŒ Error: {e} is not in vocabulary!\")\n",
                "    print(\"\\nðŸ’¡ This is why we need special tokens and better tokenization!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Enter: Byte Pair Encoding (BPE)\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Modern LLMs use Byte Pair Encoding (BPE). Instead of whole words, it breaks text into subword units. This way, even unknown words can be represented!\"\n",
                "\n",
                "ðŸ’¡ **How BPE Works:**\n",
                "1. Start with characters as tokens\n",
                "2. Find most frequent pair of tokens\n",
                "3. Merge them into a new token\n",
                "4. Repeat until desired vocabulary size\n",
                "\n",
                "```\n",
                "Example: \"lowest\" might become [\"low\", \"est\"]\n",
                "This way, we can handle \"lowest\", \"higher\", \"newest\" etc.\n",
                "with a smaller vocabulary!\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸš€ Using GPT-2's Tokenizer (tiktoken)\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Let's use the actual tokenizer that GPT-2 uses! OpenAI released it as 'tiktoken'. It's blazing fast and handles everything we need.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Vocabulary size: 50,257\n"
                    ]
                }
            ],
            "source": [
                "import tiktoken\n",
                "\n",
                "# Load GPT-2 tokenizer\n",
                "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
                "\n",
                "print(f\"Vocabulary size: {tokenizer.n_vocab:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Text: Hello, world! This is a test of GPT-2's tokenizer. ðŸš€\n",
                        "\n",
                        "Token IDs: [15496, 11, 995, 0, 770, 318, 257, 1332, 286, 402, 11571, 12, 17, 338, 11241, 7509, 13, 12520, 248, 222]\n",
                        "Number of tokens: 20\n",
                        "\n",
                        "Decoded: Hello, world! This is a test of GPT-2's tokenizer. ðŸš€\n"
                    ]
                }
            ],
            "source": [
                "# Let's test it!\n",
                "text = \"Hello, world! This is a test of GPT-2's tokenizer. ðŸš€\"\n",
                "\n",
                "# Encode\n",
                "ids = tokenizer.encode(text)\n",
                "print(f\"Text: {text}\")\n",
                "print(f\"\\nToken IDs: {ids}\")\n",
                "print(f\"Number of tokens: {len(ids)}\")\n",
                "\n",
                "# Decode back\n",
                "decoded = tokenizer.decode(ids)\n",
                "print(f\"\\nDecoded: {decoded}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Token breakdown:\n",
                        "  ID 15496 â†’ 'Hello'\n",
                        "  ID    11 â†’ ','\n",
                        "  ID   995 â†’ ' world'\n",
                        "  ID     0 â†’ '!'\n",
                        "  ID   770 â†’ ' This'\n",
                        "  ID   318 â†’ ' is'\n",
                        "  ID   257 â†’ ' a'\n",
                        "  ID  1332 â†’ ' test'\n",
                        "  ID   286 â†’ ' of'\n",
                        "  ID   402 â†’ ' G'\n",
                        "  ID 11571 â†’ 'PT'\n",
                        "  ID    12 â†’ '-'\n",
                        "  ID    17 â†’ '2'\n",
                        "  ID   338 â†’ ''s'\n",
                        "  ID 11241 â†’ ' token'\n",
                        "  ID  7509 â†’ 'izer'\n",
                        "  ID    13 â†’ '.'\n",
                        "  ID 12520 â†’ ' ï¿½'\n",
                        "  ID   248 â†’ 'ï¿½'\n",
                        "  ID   222 â†’ 'ï¿½'\n"
                    ]
                }
            ],
            "source": [
                "# Let's see what each token looks like\n",
                "print(\"Token breakdown:\")\n",
                "for idx in ids:\n",
                "    token_text = tokenizer.decode([idx])\n",
                "    print(f\"  ID {idx:5d} â†’ '{token_text}'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ðŸ’¡ **Observe:**\n",
                "> \"Notice how common words are single tokens, but the emoji gets split into multiple tokens. That's BPE in action!\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part 3: Embeddings & DataLoader (45 min)\n",
                "---\n",
                "\n",
                "## ðŸ“¢ Instructor Script\n",
                "> \"Great! We can convert text to numbers. But these are just arbitrary IDs. We need something more meaningful - **embeddings**!\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ What are Embeddings?\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"An embedding is a dense vector representation of a token. Unlike one-hot encoding, embeddings capture semantic meaning in continuous space.\"\n",
                "\n",
                "ðŸ’¡ **Key Concepts:**\n",
                "- Each token becomes a vector of N numbers (e.g., 768 dimensions)\n",
                "- Similar tokens have similar embeddings\n",
                "- These embeddings are **learned** during training!\n",
                "\n",
                "```\n",
                "Token ID: 15496 (\"Hello\")\n",
                "     â†“ (embedding lookup)\n",
                "Vector: [0.23, -0.45, 0.12, ..., 0.78]  # 768 numbers\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Embedding layer shape: torch.Size([50257, 768])\n",
                        "This means: 50,257 tokens, each with 768 dimensions\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "# GPT-2 uses:\n",
                "# - Vocabulary size: 50,257 tokens\n",
                "# - Embedding dimension: 768\n",
                "\n",
                "vocab_size = 50257\n",
                "embedding_dim = 768\n",
                "\n",
                "# Create an embedding layer\n",
                "token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
                "\n",
                "print(f\"Embedding layer shape: {token_embedding.weight.shape}\")\n",
                "print(f\"This means: {vocab_size:,} tokens, each with {embedding_dim} dimensions\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Token IDs: [15496, 995]\n",
                        "\n",
                        "Embeddings shape: torch.Size([2, 768])\n",
                        "This means: 2 tokens, each with 768 dimensions\n"
                    ]
                }
            ],
            "source": [
                "# Let's embed some tokens!\n",
                "text = \"Hello world\"\n",
                "token_ids = tokenizer.encode(text)\n",
                "print(f\"Token IDs: {token_ids}\")\n",
                "\n",
                "# Convert to tensor\n",
                "token_ids_tensor = torch.tensor(token_ids)\n",
                "\n",
                "# Get embeddings\n",
                "embeddings = token_embedding(token_ids_tensor)\n",
                "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
                "print(f\"This means: {len(token_ids)} tokens, each with {embedding_dim} dimensions\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "First token 'Hello' embedding (first 10 values):\n",
                        "tensor([ 0.6617,  0.4164, -1.1538,  0.1830,  0.4848, -0.6248, -1.2242,  0.8572,\n",
                        "        -1.0835,  0.5786], grad_fn=<SliceBackward0>)\n"
                    ]
                }
            ],
            "source": [
                "# Let's peek at the first token's embedding\n",
                "print(f\"First token 'Hello' embedding (first 10 values):\")\n",
                "print(embeddings[0, :10])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“ Positional Embeddings\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"There's one more thing! The model needs to know the **position** of each token. 'Dog bites man' is different from 'Man bites dog'!\"\n",
                "\n",
                "ðŸ’¡ **Key Insight:**\n",
                "- Attention doesn't have a built-in notion of order\n",
                "- We add positional information to token embeddings\n",
                "- GPT-2 uses **learned** positional embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Position embedding shape: torch.Size([1024, 768])\n",
                        "This means: 1024 positions, each with 768 dimensions\n"
                    ]
                }
            ],
            "source": [
                "# GPT-2 has a maximum context length of 1024 tokens\n",
                "max_length = 1024\n",
                "\n",
                "# Create positional embedding\n",
                "position_embedding = nn.Embedding(max_length, embedding_dim)\n",
                "\n",
                "print(f\"Position embedding shape: {position_embedding.weight.shape}\")\n",
                "print(f\"This means: {max_length} positions, each with {embedding_dim} dimensions\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Token embeddings shape: torch.Size([2, 768])\n",
                        "Position embeddings shape: torch.Size([2, 768])\n",
                        "Final input embeddings shape: torch.Size([2, 768])\n"
                    ]
                }
            ],
            "source": [
                "# Combine token embeddings with position embeddings\n",
                "seq_length = len(token_ids)\n",
                "positions = torch.arange(seq_length)  # [0, 1, 2, ...]\n",
                "\n",
                "# Get position embeddings\n",
                "pos_embeddings = position_embedding(positions)\n",
                "\n",
                "# Final input = token embedding + position embedding\n",
                "input_embeddings = embeddings + pos_embeddings\n",
                "\n",
                "print(f\"Token embeddings shape: {embeddings.shape}\")\n",
                "print(f\"Position embeddings shape: {pos_embeddings.shape}\")\n",
                "print(f\"Final input embeddings shape: {input_embeddings.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“¦ Building a DataLoader\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"To train an LLM, we need to create input-target pairs. The input is a sequence, and the target is the same sequence shifted by one token.\"\n",
                "\n",
                "```\n",
                "Text: \"The cat sat on the mat\"\n",
                "\n",
                "Input:  [The, cat, sat, on, the]\n",
                "Target: [cat, sat, on, the, mat]\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "class GPTDataset(Dataset):\n",
                "    \"\"\"Dataset for GPT training with sliding window.\"\"\"\n",
                "    \n",
                "    def __init__(self, text, tokenizer, max_length, stride):\n",
                "        self.input_ids = []\n",
                "        self.target_ids = []\n",
                "        \n",
                "        # Tokenize entire text\n",
                "        token_ids = tokenizer.encode(text)\n",
                "        \n",
                "        # Create sliding window samples\n",
                "        for i in range(0, len(token_ids) - max_length, stride):\n",
                "            input_chunk = token_ids[i:i + max_length]\n",
                "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
                "            self.input_ids.append(torch.tensor(input_chunk))\n",
                "            self.target_ids.append(torch.tensor(target_chunk))\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.input_ids)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        return self.input_ids[idx], self.target_ids[idx]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of samples: 5141\n",
                        "\n",
                        "First sample:\n",
                        "Input IDs:  [40, 367, 2885, 1464]\n",
                        "Target IDs: [367, 2885, 1464, 1807]\n",
                        "\n",
                        "Input text:  'I HAD always'\n",
                        "Target text: ' HAD always thought'\n"
                    ]
                }
            ],
            "source": [
                "# Create dataset from our text\n",
                "max_length = 4  # Small for demonstration\n",
                "stride = 1      # Move 1 token at a time\n",
                "\n",
                "dataset = GPTDataset(raw_text, tokenizer, max_length, stride)\n",
                "\n",
                "print(f\"Number of samples: {len(dataset)}\")\n",
                "\n",
                "# Look at first sample\n",
                "input_ids, target_ids = dataset[0]\n",
                "print(f\"\\nFirst sample:\")\n",
                "print(f\"Input IDs:  {input_ids.tolist()}\")\n",
                "print(f\"Target IDs: {target_ids.tolist()}\")\n",
                "print(f\"\\nInput text:  '{tokenizer.decode(input_ids.tolist())}'\")\n",
                "print(f\"Target text: '{tokenizer.decode(target_ids.tolist())}'\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Batch input shape: torch.Size([4, 4])\n",
                        "Batch target shape: torch.Size([4, 4])\n",
                        "\n",
                        "This means: 4 samples, each with 4 tokens\n"
                    ]
                }
            ],
            "source": [
                "# Create DataLoader for batching\n",
                "batch_size = 4\n",
                "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
                "\n",
                "# Get one batch\n",
                "inputs, targets = next(iter(dataloader))\n",
                "\n",
                "print(f\"Batch input shape: {inputs.shape}\")\n",
                "print(f\"Batch target shape: {targets.shape}\")\n",
                "print(f\"\\nThis means: {batch_size} samples, each with {max_length} tokens\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part 4: Exercises & Q&A (30 min)\n",
                "---\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Great work everyone! Now let's practice what we learned. Try these exercises:\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Exercise 1: Tokenizer Exploration\n",
                "\n",
                "Try tokenizing different types of text and observe the results:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "'Hello world'\n",
                        "  â†’ 2 tokens: [15496, 995]\n",
                        "\n",
                        "'Supercalifragilisticexpialidocious'\n",
                        "  â†’ 11 tokens: [12442, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\n",
                        "\n",
                        "'Python is awesome! ðŸ'\n",
                        "  â†’ 7 tokens: [37906, 318, 7427, 0, 12520, 238, 235]\n",
                        "\n",
                        "'The temperature is 98.6Â°F'\n",
                        "  â†’ 8 tokens: [464, 5951, 318, 9661, 13, 21, 7200, 37]\n",
                        "\n",
                        "'æ—¥æœ¬èªžãƒ†ã‚¹ãƒˆ'\n",
                        "  â†’ 8 tokens: [33768, 98, 17312, 105, 45739, 252, 24336, 43302]\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# TODO: Tokenize these different texts and compare token counts\n",
                "texts = [\n",
                "    \"Hello world\",\n",
                "    \"Supercalifragilisticexpialidocious\",\n",
                "    \"Python is awesome! ðŸ\",\n",
                "    \"The temperature is 98.6Â°F\",\n",
                "    \"æ—¥æœ¬èªžãƒ†ã‚¹ãƒˆ\"  # Japanese text\n",
                "]\n",
                "\n",
                "for text in texts:\n",
                "    tokens = tokenizer.encode(text)\n",
                "    print(f\"'{text}'\")\n",
                "    print(f\"  â†’ {len(tokens)} tokens: {tokens}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Exercise 2: Embedding Similarity\n",
                "\n",
                "âš ï¸ **Important Note:** Our embedding layer has **random, untrained weights**, so the\n",
                "similarity scores below will be essentially random noise. In a trained model (like GPT-2),\n",
                "semantically related words (e.g., \"king\" and \"queen\") would have much higher cosine\n",
                "similarity than unrelated words (e.g., \"king\" and \"apple\"). We'll see this come to life\n",
                "when we load pretrained weights in Session 4!\n",
                "\n",
                "Compare embeddings of similar words:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cosine Similarity Matrix:\n",
                        "          king   queen     man   woman   apple\n",
                        "  king 1.000  0.008  -0.029  -0.021  -0.026\n",
                        " queen 0.008  1.000  -0.002  -0.059  0.006\n",
                        "   man -0.029  -0.002  1.000  0.021  -0.093\n",
                        " woman -0.021  -0.059  0.021  1.000  0.043\n",
                        " apple -0.026  0.006  -0.093  0.043  1.000\n"
                    ]
                }
            ],
            "source": [
                "# TODO: Get embeddings for these words and compute cosine similarity\n",
                "import torch.nn.functional as F\n",
                "\n",
                "words = [\"king\", \"queen\", \"man\", \"woman\", \"apple\"]\n",
                "\n",
                "# Get token IDs\n",
                "word_ids = [tokenizer.encode(w)[0] for w in words]  # Take first token\n",
                "word_tensors = torch.tensor(word_ids)\n",
                "\n",
                "# Get embeddings (random since not trained)\n",
                "word_embeddings = token_embedding(word_tensors)\n",
                "\n",
                "# Compute cosine similarity matrix\n",
                "normalized = F.normalize(word_embeddings, dim=1)\n",
                "similarity = normalized @ normalized.T\n",
                "\n",
                "print(\"Cosine Similarity Matrix:\")\n",
                "print(\"       \", \"  \".join(f\"{w:>6}\" for w in words))\n",
                "for i, w in enumerate(words):\n",
                "    print(f\"{w:>6}\", \"  \".join(f\"{similarity[i,j]:.3f}\" for j in range(len(words))))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Exercise 3: DataLoader Visualization\n",
                "\n",
                "Visualize how the sliding window creates training samples:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original text: 'The quick brown fox jumps over the lazy dog'\n",
                        "\n",
                        "Sliding window samples (max_length=4, stride=1):\n",
                        "--------------------------------------------------\n",
                        "Sample 0: 'The quick brown fox' â†’ ' quick brown fox jumps'\n",
                        "Sample 1: ' quick brown fox jumps' â†’ ' brown fox jumps over'\n",
                        "Sample 2: ' brown fox jumps over' â†’ ' fox jumps over the'\n",
                        "Sample 3: ' fox jumps over the' â†’ ' jumps over the lazy'\n",
                        "Sample 4: ' jumps over the lazy' â†’ ' over the lazy dog'\n"
                    ]
                }
            ],
            "source": [
                "# TODO: Create a small dataset and visualize the sliding window\n",
                "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
                "sample_dataset = GPTDataset(sample_text, tokenizer, max_length=4, stride=1)\n",
                "\n",
                "print(f\"Original text: '{sample_text}'\")\n",
                "print(f\"\\nSliding window samples (max_length=4, stride=1):\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "for i in range(min(5, len(sample_dataset))):\n",
                "    inp, tgt = sample_dataset[i]\n",
                "    inp_text = tokenizer.decode(inp.tolist())\n",
                "    tgt_text = tokenizer.decode(tgt.tolist())\n",
                "    print(f\"Sample {i}: '{inp_text}' â†’ '{tgt_text}'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ðŸ“ Session 1 Summary\n",
                "\n",
                "ðŸ“¢ **Say this:**\n",
                "> \"Excellent work today! Let's recap what we learned:\"\n",
                "\n",
                "### Key Takeaways:\n",
                "\n",
                "1. **LLMs predict the next token** - that's the core idea!\n",
                "\n",
                "2. **Tokenization** converts text to numbers:\n",
                "   - Word-level â†’ vocabulary explosion\n",
                "   - BPE (subword) â†’ handles unknown words\n",
                "   - GPT-2 uses tiktoken with 50,257 tokens\n",
                "\n",
                "3. **Embeddings** give tokens meaning:\n",
                "   - Token embeddings: learned representations\n",
                "   - Position embeddings: where in sequence\n",
                "   - Final input = token + position embedding\n",
                "\n",
                "4. **DataLoader** creates training pairs:\n",
                "   - Input: sequence of tokens\n",
                "   - Target: same sequence shifted by 1\n",
                "\n",
                "### Next Session:\n",
                "> \"This afternoon, we'll dive into the heart of transformers - **Attention Mechanisms**. Get ready to understand how the model 'pays attention' to different parts of the input!\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ðŸ”— Resources\n",
                "\n",
                "- [LLMs from Scratch GitHub](https://github.com/rasbt/LLMs-from-scratch)\n",
                "- [Book: Build a Large Language Model From Scratch](http://mng.bz/orYv)\n",
                "- [tiktoken documentation](https://github.com/openai/tiktoken)\n",
                "- [Attention Is All You Need paper](https://arxiv.org/abs/1706.03762)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
