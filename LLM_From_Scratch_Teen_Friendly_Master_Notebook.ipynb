{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs From Scratch: Visual + Interactive Master Notebook (v3)\n",
    "\n",
    "This version adds **step-by-step visualizations** and **interactive knobs** (change variables and re-run cells) so beginners can see exactly how each part works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Path\n",
    "1. Math intuition with plots\n",
    "2. Tokenization and embedding visuals\n",
    "3. Attention heatmaps\n",
    "4. Causal masking visualization\n",
    "5. Tiny GPT build\n",
    "6. Interactive generation controls\n",
    "7. Training curve visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 4)\n",
    "torch.manual_seed(42)\n",
    "print('Ready. Change variables in cells to explore interactively.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A \u2014 Math Visuals\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step A1: Dot-product as similarity\n",
    "v1 = torch.tensor([2.0, 4.0])\n",
    "v2 = torch.tensor([1.0, 2.0])\n",
    "v3 = torch.tensor([-2.0, -4.0])\n",
    "\n",
    "pairs = [('v1\u00b7v2', torch.dot(v1,v2).item()), ('v1\u00b7v3', torch.dot(v1,v3).item())]\n",
    "labels = [p[0] for p in pairs]\n",
    "values = [p[1] for p in pairs]\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.title('Dot Product Comparison')\n",
    "plt.axhline(0, color='black', lw=1)\n",
    "plt.show()\n",
    "print('Interpretation: positive=aligned, negative=opposite direction')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step A2: Softmax temperature visualization (interactive)\n",
    "scores = torch.tensor([3.0, 1.0, 0.2, -1.0])\n",
    "temperatures = [2.0, 1.0, 0.5, 0.2]  # try your own values\n",
    "\n",
    "fig, axes = plt.subplots(1, len(temperatures), figsize=(14,3), sharey=True)\n",
    "for ax, T in zip(axes, temperatures):\n",
    "    probs = torch.softmax(scores / T, dim=0)\n",
    "    ax.bar(range(len(scores)), probs.numpy())\n",
    "    ax.set_title(f'T={T}')\n",
    "    ax.set_xticks(range(len(scores)))\n",
    "plt.suptitle('Lower temperature => sharper distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B \u2014 Tokenization & Embeddings Visuals\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"llms can learn patterns from data very quickly\"\n",
    "tokens = text.split()\n",
    "vocab = {w:i for i,w in enumerate(sorted(set(tokens)))}\n",
    "ids = [vocab[t] for t in tokens]\n",
    "\n",
    "print('Tokens:', tokens)\n",
    "print('IDs   :', ids)\n",
    "\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.scatter(range(len(ids)), [1]*len(ids), s=120)\n",
    "for i, (tok, tid) in enumerate(zip(tokens, ids)):\n",
    "    plt.text(i, 1.02, f'{tok}\n",
    "{tid}', ha='center', va='bottom', fontsize=9)\n",
    "plt.yticks([])\n",
    "plt.title('Token -> ID mapping along sequence')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Embedding geometry projection (2D via first 2 dims for teaching)\n",
    "vocab_size = max(vocab.values()) + 1\n",
    "d_model = 8\n",
    "emb = nn.Embedding(vocab_size, d_model)\n",
    "E = emb.weight.detach()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(E[:,0].numpy(), E[:,1].numpy())\n",
    "for token, idx in vocab.items():\n",
    "    plt.text(E[idx,0].item(), E[idx,1].item(), token)\n",
    "plt.title('Embedding vectors (first 2 dimensions)')\n",
    "plt.xlabel('dim0'); plt.ylabel('dim1')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C \u2014 Attention: Step-by-Step + Heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scaled_dot_attention(Q, K, V, mask=None):\n",
    "    d_k = Q.size(-1)\n",
    "    scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    out = weights @ V\n",
    "    return out, weights\n",
    "\n",
    "# toy sequence with token labels\n",
    "token_labels = ['I', 'love', 'learning', 'AI']\n",
    "B, T, C = 1, len(token_labels), 8\n",
    "X = torch.randn(B, T, C)\n",
    "Wq, Wk, Wv = [torch.randn(C, C) for _ in range(3)]\n",
    "Q, K, V = X @ Wq, X @ Wk, X @ Wv\n",
    "out, attn = scaled_dot_attention(Q, K, V)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(attn[0].detach().numpy(), cmap='viridis')\n",
    "plt.colorbar(label='attention weight')\n",
    "plt.xticks(range(T), token_labels)\n",
    "plt.yticks(range(T), token_labels)\n",
    "plt.title('Attention Heatmap (who looks at whom)')\n",
    "plt.xlabel('Key token'); plt.ylabel('Query token')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D \u2014 Causal Mask Visualization (GPT rule: no future peeking)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "T = 8\n",
    "causal_mask = torch.tril(torch.ones(T, T))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(causal_mask.numpy(), cmap='gray_r')\n",
    "plt.title('Causal Mask (white=visible, black=blocked)')\n",
    "plt.xlabel('Key position'); plt.ylabel('Query position')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E \u2014 Build Tiny Transformer + GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TinyBlock(nn.Module):\n",
    "    def __init__(self, d_model=32, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        a, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
    "        x = self.ln1(x + a)\n",
    "        f = self.ffn(x)\n",
    "        return self.ln2(x + f)\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size=200, d_model=64, n_layers=2, n_heads=4, max_len=64):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Embedding(max_len, d_model)\n",
    "        self.blocks = nn.ModuleList([TinyBlock(d_model, n_heads) for _ in range(n_layers)])\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
    "        x = self.tok(idx) + self.pos(pos)\n",
    "        causal = torch.triu(torch.ones(T, T, device=idx.device), diagonal=1).bool()\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask=causal)\n",
    "        return self.head(self.ln(x))\n",
    "\n",
    "model = TinyGPT(vocab_size=120, d_model=32, n_layers=2, n_heads=4, max_len=64)\n",
    "print('Model ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part F \u2014 Visual Training Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "losses = []\n",
    "\n",
    "steps = 30  # increase to 100+ for smoother trend\n",
    "for step in range(steps):\n",
    "    x = torch.randint(0, 120, (16, 20))\n",
    "    y = x.roll(-1, dims=1)\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits[:, :-1, :].reshape(-1, 120), y[:, :-1].reshape(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss Curve (interactive: change steps/lr)')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "print('Final loss:', round(losses[-1], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part G \u2014 Interactive Generation Controls\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start_ids, max_new_tokens=20, temperature=1.0, top_k=None):\n",
    "    model.eval()\n",
    "    idx = start_ids.clone()\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(idx)[:, -1, :] / temperature\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, k=top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx\n",
    "\n",
    "start = torch.randint(0, 120, (1, 6))\n",
    "\n",
    "# Interactive knobs: change and re-run\n",
    "MAX_NEW = 12\n",
    "TEMP = 0.8\n",
    "TOP_K = 20\n",
    "\n",
    "sample = generate(model, start, max_new_tokens=MAX_NEW, temperature=TEMP, top_k=TOP_K)\n",
    "print('Generated token IDs:', sample.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part H \u2014 Quick Challenges (Visual)\n",
    "1. Change `temperatures` and explain probability shape change.\n",
    "2. Change `TOP_K` and compare generation diversity.\n",
    "3. Increase training `steps` and compare loss curve.\n",
    "4. Replace random training data with your own tokenized text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responsible AI Reminder\n",
    "- Generated text can be fluent but wrong.\n",
    "- Check for bias and unsafe outputs.\n",
    "- Human review is important in real products.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}